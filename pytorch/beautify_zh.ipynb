{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4529c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 記得我們是使用中文 BERT\n",
    "model_version = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "magic_threshold = 0.6\n",
    "\n",
    "# 情境句子\n",
    "text_a = \"[CLS]等到潮水[MASK]了,就知道誰沒穿褲子\"\n",
    "text_b = \"等到潮水[MASK]了\"\n",
    "text_c = \"就知道誰沒穿褲子\"\n",
    "text_d = \"印度面臨「海嘯式」的新冠疫情，確診及死亡人數節節上升。剛離開印度回國的中國人蒙姐近日就其所見所聞娓娓道來，她認為，專家預測印度實際感染人數要比公佈的數字多3至5倍應是真的[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]在印度即使確診了也根本沒有溯源一說，沒有任何強制性核酸檢測，而據她觀察當地很多人對生死看得特別淡，心態與中國人大不同。\"\n",
    "text_f = \"印度面臨「海嘯式」的新冠疫情，確診及死亡人數節節上升。剛離開印度回國的中國人蒙姐近日就其所見所聞娓娓道來，她認為，專家預測印度實際感染人數要比公佈的數字多3至5倍應是真的。在印度即使確診了也根本沒有溯源一說，沒有任何強制性核酸檢測，而據她觀察當地很多人對生死看得特別淡，心態與中國人大不同。\"\n",
    "#\n",
    "def get_input_from_mask_sentence(a):\n",
    "    tokens = tokenizer.tokenize(a)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # 除了 tokens 以外我們還需要辨別句子的 segment ids\n",
    "    input_ids = torch.tensor([ids])  # (1, seq_len)\n",
    "    token_type_ids = torch.zeros_like(input_ids)  # (1, seq_len)\n",
    "    return tokens, input_ids ,token_type_ids\n",
    "\n",
    "#\n",
    "def get_input_from_two_sentence(a, b):\n",
    "    inputs = tokenizer.encode_plus(a, b, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids ,token_type_ids = inputs['input_ids'],inputs['token_type_ids']\n",
    "    \n",
    "    return input_ids ,token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1a8c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "印 印\n",
      "度 度\n",
      "面 面\n",
      "臨 臨\n",
      "「 「\n",
      "海 海\n",
      "嘯 嘯\n",
      "式 式\n",
      "」 」\n",
      "的 的\n",
      "新 新\n",
      "冠 冠\n",
      "疫 疫\n",
      "情 情\n",
      "， ，\n",
      "確 確\n",
      "診 診\n",
      "及 及\n",
      "死 死\n",
      "亡 亡\n",
      "人 人\n",
      "數 數\n",
      "節 節\n",
      "節 節\n",
      "上 上\n",
      "升 升\n",
      "。 。\n",
      "剛 剛\n",
      "離 離\n",
      "開 開\n",
      "印 印\n",
      "度 度\n",
      "回 回\n",
      "國 來  v ->0.9020340442657471\n",
      "的 的\n",
      "中 中\n",
      "國 國\n",
      "人 人\n",
      "蒙 蒙\n",
      "姐 姐\n",
      "近 近\n",
      "日 日\n",
      "就 將  v ->0.602899968624115\n",
      "其 其\n",
      "所 所\n",
      "見 見\n",
      "所 所\n",
      "聞 聞\n",
      "娓 娓\n",
      "娓 娓\n",
      "道 道\n",
      "來 來\n",
      "， ，\n",
      "她 她\n",
      "認 認\n",
      "為 為\n",
      "， ，\n",
      "專 專\n",
      "家 家\n",
      "預 預\n",
      "測 測\n",
      "印 印\n",
      "度 度\n",
      "實 實\n",
      "際 際\n",
      "感 感\n",
      "染 染\n",
      "人 人\n",
      "數 數\n",
      "要 要\n",
      "比 比\n",
      "公 公\n",
      "佈 佈\n",
      "的 的\n",
      "數 數\n",
      "字 字\n",
      "多 多\n",
      "3 3\n",
      "至 至\n",
      "5 5\n",
      "倍 倍\n",
      "應 應\n",
      "是 是\n",
      "真 真\n",
      "的 的\n",
      "。 。\n",
      "在 在\n",
      "印 印\n",
      "度 度\n",
      "即 即\n",
      "使 使\n",
      "確 確\n",
      "診 診\n",
      "了 了\n",
      "也 也\n",
      "根 根\n",
      "本 本\n",
      "沒 沒\n",
      "有 有\n",
      "溯 溯\n",
      "源 源\n",
      "一 一\n",
      "說 說\n",
      "， ，\n",
      "沒 沒\n",
      "有 有\n",
      "任 任\n",
      "何 何\n",
      "強 強\n",
      "制 制\n",
      "性 性\n",
      "核 核\n",
      "酸 酸\n",
      "檢 檢\n",
      "測 測\n",
      "， ，\n",
      "而 而\n",
      "據 據\n",
      "她 她\n",
      "觀 觀\n",
      "察 察\n",
      "當 當\n",
      "地 地\n",
      "很 很\n",
      "多 多\n",
      "人 人\n",
      "對 對\n",
      "生 生\n",
      "死 死\n",
      "看 看\n",
      "得 得\n",
      "特 特\n",
      "別 別\n",
      "淡 淡\n",
      "， ，\n",
      "心 心\n",
      "態 態\n",
      "與 與\n",
      "中 中\n",
      "國 國\n",
      "人 人\n",
      "大 大\n",
      "不 不\n",
      "同 同\n",
      "。 。\n"
     ]
    }
   ],
   "source": [
    "# 潤句1:一次偵測\n",
    "# 適合用於微調語法\n",
    "def convert2text(predictions,k=3):\n",
    "    probs, indices = torch.topk(torch.softmax(predictions, -1), k)\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "    return predicted_tokens,probs\n",
    "\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "#\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(model_version)\n",
    "clear_output()\n",
    "\n",
    "tokens, input_ids ,token_type_ids = get_input_from_mask_sentence(text_f)\n",
    "\n",
    "# 使用 masked LM 估計 [MASK] 位置所代表的實際 token \n",
    "maskedLM_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = maskedLM_model(input_ids, token_type_ids=token_type_ids)\n",
    "    predictions = outputs[0]\n",
    "    # (1, seq_len, num_hidden_units)\n",
    "del maskedLM_model\n",
    "\n",
    "k=3\n",
    "for idx in range(0,len(tokens)):\n",
    "    predicted_tokens,probs = convert2text(predictions[0,idx],k=k)\n",
    "    boolresult= False\n",
    "    for tmp_p,tmp_t in zip(probs,predicted_tokens):\n",
    "        #\n",
    "        if tokens[idx] == tmp_t:\n",
    "            continue\n",
    "        #\n",
    "        if float(tmp_p) > magic_threshold or tokens[idx]==\"[MASK]\":\n",
    "            print(tokens[idx],tmp_t,f\" v ->{tmp_p}\")\n",
    "            boolresult = True\n",
    "            break\n",
    "    if boolresult is False:\n",
    "        print(tokens[idx],tokens[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c4398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "印,印\n",
      "度,度\n",
      "面,面\n",
      "臨,臨\n",
      "「,「\n",
      "海,海\n",
      "嘯,嘯\n",
      "式,式\n",
      "」,」\n",
      "的,的\n",
      "新,新\n",
      "冠,冠\n",
      "疫,疫\n",
      "情,情\n",
      "，,，\n",
      "確,確\n",
      "診,診\n",
      "及,及\n",
      "死,死\n",
      "亡,亡\n",
      "人,人\n",
      "數,數\n",
      "節,節\n",
      "節,節\n",
      "上,上\n",
      "升,升\n",
      "。,。\n",
      "剛,剛\n",
      "離,離\n",
      "開,開\n",
      "印,印\n",
      "度,度\n",
      "回,回\n",
      "國,國\n",
      "的,的\n",
      "中,中\n",
      "國,國\n",
      "人,人\n",
      "蒙,蒙\n",
      "姐,姐\n",
      "近,近\n",
      "日,日\n",
      "就,就\n",
      "其,其\n",
      "所,所\n",
      "見,見\n",
      "所,所\n",
      "聞,聞\n",
      "娓,娓\n",
      "娓,娓\n",
      "道,道\n",
      "來,來\n",
      "，,，\n",
      "她,她\n",
      "認,認\n",
      "為,為\n",
      "，,，\n",
      "專,專\n",
      "家,家\n",
      "預,預\n",
      "測,測\n",
      "印,印\n",
      "度,度\n",
      "實,實\n",
      "際,際\n",
      "感,感\n",
      "染,染\n",
      "人,人\n",
      "數,數\n",
      "要,要\n",
      "比,比\n",
      "公,公\n",
      "佈,佈\n",
      "的,的\n",
      "數,數\n",
      "字,字\n",
      "多,多\n",
      "3,3\n",
      "至,至\n",
      "5,5\n",
      "倍,倍\n",
      "應,應\n",
      "是,是\n",
      "真,真\n",
      "的,的\n",
      "[MASK],。 --> tensor([0.4508])\n",
      "[MASK],他 --> tensor([0.1258])\n",
      "[MASK],說 --> tensor([0.1806])\n",
      "[MASK],， --> tensor([0.3064])\n",
      "[MASK],我 --> tensor([0.0894])\n",
      "[MASK],們 --> tensor([0.3087])\n",
      "[MASK],所 --> tensor([0.1716])\n",
      "[MASK],見 --> tensor([0.2135])\n",
      "[MASK],所 --> tensor([0.6250])\n",
      "[MASK],聞 --> tensor([0.8029])\n",
      "在,在\n",
      "印,印\n",
      "度,度\n",
      "即,即\n",
      "使,使\n",
      "確,確\n",
      "診,診\n",
      "了,了\n",
      "也,也\n",
      "根,根\n",
      "本,本\n",
      "沒,沒\n",
      "有,有\n",
      "溯,溯\n",
      "源,源\n",
      "一,一\n",
      "說,說\n",
      "，,，\n",
      "沒,沒\n",
      "有,有\n",
      "任,任\n",
      "何,何\n",
      "強,強\n",
      "制,制\n",
      "性,性\n",
      "核,核\n",
      "酸,酸\n",
      "檢,檢\n",
      "測,測\n",
      "，,，\n",
      "而,而\n",
      "據,據\n",
      "她,她\n",
      "觀,觀\n",
      "察,察\n",
      "當,當\n",
      "地,地\n",
      "很,很\n",
      "多,多\n",
      "人,人\n",
      "對,對\n",
      "生,生\n",
      "死,死\n",
      "看,看\n",
      "得,得\n",
      "特,特\n",
      "別,別\n",
      "淡,淡\n",
      "，,，\n",
      "心,心\n",
      "態,態\n",
      "與,與\n",
      "中,中\n",
      "國,國\n",
      "人,人\n",
      "大,大\n",
      "不,不\n",
      "同,同\n",
      "。,。\n"
     ]
    }
   ],
   "source": [
    "# 潤句2：逐字偵測\n",
    "# 適合用於產生銜接句\n",
    "def convert2text(predictions,k=1):\n",
    "    probs, indices = torch.topk(torch.softmax(predictions, -1), k)\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "    return predicted_tokens,probs\n",
    "\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "#\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(model_version)\n",
    "clear_output()\n",
    "\n",
    "tokens, input_ids ,token_type_ids = get_input_from_mask_sentence(text_d)\n",
    "for idx,tmp_token in enumerate(tokens):\n",
    "\n",
    "    if tokens[idx]==\"[MASK]\":\n",
    "        tmp_text = tokens[:]\n",
    "        tmp_text[idx]=\"[MASK]\"\n",
    "        tmp_tokens, tmp_input_ids ,tmp_token_type_ids = get_input_from_mask_sentence(\"\".join(tmp_text))\n",
    "        with torch.no_grad():\n",
    "            outputs = maskedLM_model(tmp_input_ids, token_type_ids=tmp_token_type_ids)\n",
    "            predictions = outputs[0]\n",
    "            # (1, seq_len, num_hidden_units)\n",
    "            pred_token,prob = convert2text(predictions[0,idx])\n",
    "        print(f\"{tokens[idx]},{pred_token[0]} --> {prob}\")\n",
    "        tokens[idx] = str(pred_token[0])\n",
    "    else:\n",
    "        print(f\"{tokens[idx]},{tokens[idx]}\")\n",
    "del maskedLM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 潤句3：逐字偵測\n",
    "# 適合用於產生銜接句+潤句\n",
    "def convert2text(predictions,k=1):\n",
    "    probs, indices = torch.topk(torch.softmax(predictions, -1), k)\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "    return predicted_tokens,probs\n",
    "\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "#\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(model_version)\n",
    "clear_output()\n",
    "\n",
    "tokens, input_ids ,token_type_ids = get_input_from_mask_sentence(text_d)\n",
    "for idx,tmp_token in enumerate(tokens):\n",
    "    tmp_text = tokens[:]\n",
    "    tmp_text[idx]=\"[MASK]\"\n",
    "    tmp_tokens, tmp_input_ids ,tmp_token_type_ids = get_input_from_mask_sentence(\"\".join(tmp_text))\n",
    "    with torch.no_grad():\n",
    "        outputs = maskedLM_model(tmp_input_ids, token_type_ids=tmp_token_type_ids)\n",
    "        predictions = outputs[0]\n",
    "        # (1, seq_len, num_hidden_units)\n",
    "    pred_token,prob = convert2text(predictions[0,idx])\n",
    "    if prob[0] > magic_threshold and tokens[idx]!=pred_token[0]:\n",
    "        print(f\"{tokens[idx]},{pred_token[0]} --> {prob}\")\n",
    "        tokens[idx] = str(pred_token[0])\n",
    "    elif tokens[idx]==\"[MASK]\":\n",
    "        print(f\"{tokens[idx]},{pred_token[0]} --> {prob}\")\n",
    "        tokens[idx] = str(pred_token[0])\n",
    "    else:\n",
    "        print(f\"{tokens[idx]},{tokens[idx]}\")\n",
    "del maskedLM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ecd97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
