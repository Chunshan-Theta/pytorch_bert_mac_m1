{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Language Translation with TorchText\n",
    "===================================\n",
    "\n",
    "This tutorial shows how to use ``torchtext`` to preprocess\n",
    "data from a well-known dataset containing sentences in both English and German and use it to\n",
    "train a sequence-to-sequence model with attention that can translate German sentences\n",
    "into English.\n",
    "\n",
    "It is based off of\n",
    "`this tutorial <https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__\n",
    "from PyTorch community member `Ben Trevett <https://github.com/bentrevett>`__\n",
    "with Ben's permission. We update the tutorials by removing some legacy code.\n",
    "\n",
    "By the end of this tutorial, you will be able to preprocess sentences into tensors for NLP modeling and use `torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__ for training and validing the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing\n",
    "----------------\n",
    "``torchtext`` has utilities for creating datasets that can be easily\n",
    "iterated through for the purposes of creating a language translation\n",
    "model. In this example, we show how to tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor.\n",
    "\n",
    "Note: the tokenization in this tutorial requires `Spacy <https://spacy.io>`__\n",
    "We use Spacy because it provides strong support for tokenization in languages\n",
    "other than English. ``torchtext`` provides a ``basic_english`` tokenizer\n",
    "and supports other tokenizers for English (e.g.\n",
    "`Moses <https://bitbucket.org/luismsgomes/mosestokenizer/src/default/>`__)\n",
    "but for language translation - where multiple languages are required -\n",
    "Spacy is your best bet.\n",
    "\n",
    "To run this tutorial, first install ``spacy`` using ``pip`` or ``conda``.\n",
    "Next, download the raw data for the English and German Spacy tokenizers:\n",
    "\n",
    "::\n",
    "\n",
    "   python -m spacy download en\n",
    "   python -m spacy download de\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.354 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.354 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美国 缓慢 地 开始 倾听 ， 但 并非 没有 艰难曲折 。\n",
      "Slowly and not without struggle, America began to listen. 美国 缓慢 地 开始 倾听 ， 但 并非 没有 艰难曲折 。\n",
      "['Slowly', 'and', 'not', 'without', 'struggle', ',', 'America', 'began', 'to', 'listen', '.'] ['美国', '缓慢', '地', '开始', '倾听', '，', '但', '并非', '没有', '艰难曲折', '。']\n",
      "[(tensor([    0,     0,     0, 24580,     0,    10,   655,     0,     0,     0,\n",
      "            4]), tensor([8977,    8,   38,  255, 3369,    6,  663,  699,    9, 2059,    5]))]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "zh_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "              counter.update(tokenizer(string_))\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "def build_vocab_from_strs(strs, tokenizer):\n",
    "    counter = Counter()\n",
    "    for string_ in strs:\n",
    "        counter.update(tokenizer(string_))\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "# de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "# en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "# def data_process(filepaths):\n",
    "#   raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "#   raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "#   data = []\n",
    "#   for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "#     de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "#                             dtype=torch.long)\n",
    "#     en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "#                             dtype=torch.long)\n",
    "#     print((de_tensor_, en_tensor_))\n",
    "#     data.append((de_tensor_, en_tensor_))\n",
    "#   return data\n",
    "\n",
    "import json \n",
    "import jieba\n",
    "english_rowdata = []\n",
    "chinese_rowdata = []\n",
    "\n",
    "with open(\"./translation2019zh_valid.json\") as f:\n",
    "    for l in f.readlines():\n",
    "        json_ = json.loads(l)\n",
    "        chinese_rowdata.append(json_[\"chinese\"])\n",
    "        english_rowdata.append(json_[\"english\"])\n",
    "        \n",
    "zh_vocab = build_vocab_from_strs(chinese_rowdata, zh_tokenizer)\n",
    "en_vocab = build_vocab_from_strs(english_rowdata, en_tokenizer)\n",
    "\n",
    "data = []      \n",
    "for e,c in zip(english_rowdata,chinese_rowdata):\n",
    "    c = \" \".join(jieba.cut(c))\n",
    "    print(c)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(e)],\n",
    "                            dtype=torch.long)\n",
    "    zh_tensor_ = torch.tensor([zh_vocab[token] for token in zh_tokenizer(c)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((zh_tensor_, en_tensor_))\n",
    "    #\n",
    "    print(e,c)\n",
    "    print(zh_tokenizer(e),en_tokenizer(c))\n",
    "    break\n",
    "print(data)\n",
    "\n",
    "# train_data = data_process(train_filepaths)\n",
    "# val_data = data_process(val_filepaths)\n",
    "# test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``DataLoader``\n",
    "----------------\n",
    "The last ``torch`` specific feature we'll use is the ``DataLoader``,\n",
    "which is easy to use since it takes the data as its\n",
    "first argument. Specifically, as the docs say:\n",
    "``DataLoader`` combines a dataset and a sampler, and provides an iterable over the given dataset. The ``DataLoader`` supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning. \n",
    "\n",
    "Please pay attention to ``collate_fn`` (optional) that merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "  return de_batch, en_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our ``nn.Module`` and ``Optimizer``\n",
    "----------------\n",
    "That's mostly it from a ``torchtext`` perspecive: with the dataset built\n",
    "and the iterator defined, the rest of this tutorial simply defines our\n",
    "model as an ``nn.Module``, along with an ``Optimizer``, and then trains it.\n",
    "\n",
    "Our model specifically, follows the architecture described\n",
    "`here <https://arxiv.org/abs/1409.0473>`__ (you can find a\n",
    "significantly more commented version\n",
    "`here <https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__).\n",
    "\n",
    "Note: this model is just an example model that can be used for language\n",
    "translation; we choose it because it is a standard model for the task,\n",
    "not because it is the recommended model to use for translation. As you're\n",
    "likely aware, state-of-the-art models are currently based on Transformers;\n",
    "you can see PyTorch's capabilities for implementing Transformer layers\n",
    "`here <https://pytorch.org/docs/stable/nn.html#transformer-layers>`__; and\n",
    "in particular, the \"attention\" used in the model below is different from\n",
    "the multi-headed self-attention present in a transformer model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,491,070 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: int,\n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "\n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "\n",
    "        return weighted_encoder_rep\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input: Tensor,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_encoder_rep,\n",
    "                                     embedded), dim = 1))\n",
    "\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: when scoring the performance of a language translation model in\n",
    "particular, we have to tell the ``nn.CrossEntropyLoss`` function to\n",
    "ignore the indices where the target is simply padding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = en_vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train and evaluate this model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 50s\n",
      "\tTrain Loss: 3.483 | Train PPL:  32.548\n",
      "\t Val. Loss: 4.225 |  Val. PPL:  68.357\n",
      "Epoch: 02 | Time: 0m 50s\n",
      "\tTrain Loss: 3.379 | Train PPL:  29.349\n",
      "\t Val. Loss: 4.130 |  Val. PPL:  62.178\n",
      "Epoch: 03 | Time: 0m 50s\n",
      "\tTrain Loss: 3.287 | Train PPL:  26.762\n",
      "\t Val. Loss: 4.129 |  Val. PPL:  62.094\n",
      "Epoch: 04 | Time: 0m 50s\n",
      "\tTrain Loss: 3.209 | Train PPL:  24.743\n",
      "\t Val. Loss: 4.052 |  Val. PPL:  57.535\n",
      "Epoch: 05 | Time: 0m 50s\n",
      "\tTrain Loss: 3.126 | Train PPL:  22.772\n",
      "\t Val. Loss: 4.031 |  Val. PPL:  56.329\n",
      "Epoch: 06 | Time: 0m 50s\n",
      "\tTrain Loss: 3.066 | Train PPL:  21.455\n",
      "\t Val. Loss: 3.979 |  Val. PPL:  53.447\n",
      "Epoch: 07 | Time: 0m 50s\n",
      "\tTrain Loss: 3.019 | Train PPL:  20.481\n",
      "\t Val. Loss: 3.926 |  Val. PPL:  50.684\n",
      "Epoch: 08 | Time: 0m 50s\n",
      "\tTrain Loss: 2.930 | Train PPL:  18.720\n",
      "\t Val. Loss: 3.918 |  Val. PPL:  50.307\n",
      "Epoch: 09 | Time: 0m 50s\n",
      "\tTrain Loss: 2.901 | Train PPL:  18.199\n",
      "\t Val. Loss: 3.857 |  Val. PPL:  47.320\n",
      "Epoch: 10 | Time: 0m 50s\n",
      "\tTrain Loss: 2.839 | Train PPL:  17.104\n",
      "\t Val. Loss: 3.845 |  Val. PPL:  46.761\n",
      "Epoch: 11 | Time: 0m 51s\n",
      "\tTrain Loss: 2.798 | Train PPL:  16.409\n",
      "\t Val. Loss: 3.834 |  Val. PPL:  46.225\n",
      "Epoch: 12 | Time: 0m 50s\n",
      "\tTrain Loss: 2.752 | Train PPL:  15.672\n",
      "\t Val. Loss: 3.813 |  Val. PPL:  45.296\n",
      "Epoch: 13 | Time: 0m 50s\n",
      "\tTrain Loss: 2.697 | Train PPL:  14.842\n",
      "\t Val. Loss: 3.796 |  Val. PPL:  44.520\n",
      "Epoch: 14 | Time: 0m 50s\n",
      "\tTrain Loss: 2.657 | Train PPL:  14.257\n",
      "\t Val. Loss: 3.795 |  Val. PPL:  44.483\n",
      "Epoch: 15 | Time: 0m 50s\n",
      "\tTrain Loss: 2.622 | Train PPL:  13.756\n",
      "\t Val. Loss: 3.793 |  Val. PPL:  44.371\n",
      "Epoch: 16 | Time: 0m 50s\n",
      "\tTrain Loss: 2.576 | Train PPL:  13.147\n",
      "\t Val. Loss: 3.795 |  Val. PPL:  44.468\n",
      "Epoch: 17 | Time: 0m 50s\n",
      "\tTrain Loss: 2.550 | Train PPL:  12.808\n",
      "\t Val. Loss: 3.777 |  Val. PPL:  43.694\n",
      "Epoch: 18 | Time: 0m 50s\n",
      "\tTrain Loss: 2.519 | Train PPL:  12.417\n",
      "\t Val. Loss: 3.740 |  Val. PPL:  42.088\n",
      "Epoch: 19 | Time: 0m 50s\n",
      "\tTrain Loss: 2.489 | Train PPL:  12.049\n",
      "\t Val. Loss: 3.743 |  Val. PPL:  42.239\n",
      "Epoch: 20 | Time: 0m 50s\n",
      "\tTrain Loss: 2.456 | Train PPL:  11.654\n",
      "\t Val. Loss: 3.750 |  Val. PPL:  42.527\n",
      "Epoch: 21 | Time: 0m 50s\n",
      "\tTrain Loss: 2.444 | Train PPL:  11.514\n",
      "\t Val. Loss: 3.713 |  Val. PPL:  40.971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b177e7f2de2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b177e7f2de2b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "model.load_state_dict(torch.load(\"./save_de2en-9.pt\"))\n",
    "def train(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "    # Save:\n",
    "    if epoch % 3 == 0:\n",
    "        torch.save(model.state_dict(), f\"./save_de2en-{epoch}.pt\")\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps\n",
    "--------------\n",
    "\n",
    "- Check out the rest of Ben Trevett's tutorials using ``torchtext``\n",
    "  `here <https://github.com/bentrevett/>`__\n",
    "- Stay tuned for a tutorial using other ``torchtext`` features along\n",
    "  with ``nn.Transformer`` for language modeling via next word prediction!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mann', '<unk>', 'Cowboy', 'Mann', 'Mädchen', 'Frau', 'Mann', 'Mädchen', 'Deutsche', 'Jungen', 'gehen', 'Männer', 'Jungen', 'kämpfen', 'marschieren', 'Männer', '#', 'schwarz-weißer', 'Person', 'Mann', 'einem', 'Person', 'kleines', 'Cowboy', 'Mann', 'sitzen', 'spielen', 'Reh', 'Frau', 'Mann', 'Mann', 'Mann', 'Arbeiter', 'kleines', 'Gruppe', 'Leute', 'Straße', 'männliche', 'Mann', 'hell', 'Junge', 'kleiner', 'Kinder', 'Mann', 'Turnerin', 'Mann', 'älterer', 'Junge', 'Frau', 'Wachmann', 'Baby', 'Mann', 'Frauen', 'Springen', 'Frau', '<unk>', 'Asiate', 'Männer', 'Mann', 'Frau', 'Männer', 'einzelne', 'brauner', '<unk>', 'Bauarbeiter', 'gehen', 'sitzender', 'junges', 'Mann', 'Mann', 'kleines', 'kleines', 'junges', 'Junge', 'Katze', 'Mann', 'bunt', 'zwei', 'ist', 'Frau', 'Junge', 'kleiner', 'Jungen', 'Herr', 'sitzen', ',', 'Männer', 'Mann', 'Frau', 'Mutter', 'Mann', 'Männer', 'Mann', 'Personen', 'Spieler', 'Frau', 'Footballspieler', 'Teenager', 'Mann', 'Hund', 'Frau', 'afrikanischer', '<unk>', 'Mann', 'Gruppe', 'kleines', 'Mann', 'Frauen', 'Personen', 'Männer', 'Mann', 'Mann', 'Leute', 'Baseballspieler', 'große', 'Asiate', 'Frau', 'Kinder', 'Bergsteiger', 'Typ', 'professionell', 'Junge', 'Männer', 'Mann', '<unk>', 'Männer', 'Dame', '<unk>']\n",
      "['man', 'class', 'cowboy', 'middle', 'girl', 'woman', 'man', 'girl', 'German', 'boys', 'are', 'men', 'boys', 'compete', '<unk>', 'men', '8', 'black', 'person', 'man', 'photo', 'person', 'small', 'cowboy', 'man', 'sitting', 'play', 'deer', 'woman', 'middle', 'man', 'man', 'workers', 'little', 'group', 'people', 'road', 'male', 'man', 'man', 'boy', 'young', 'children', 'carrying', 'gymnast', 'man', 'elderly', 'boy', 'woman', 'guard', 'baby', 'man', 'women', 'jump', 'woman', '<unk>', 'Asian', 'men', 'man', 'woman', 'men', 'solitary', 'brown', 'busy', 'construction', 'are', 'sitting', 'young', 'man', 'man', 'toddler', 'little', 'girl', 'in', 'cat', 'man', 'colorfully', 'two', 'is', 'woman', 'boy', 'small', 'boys', 'gentleman', 'sit', 'playing', 'men', 'shirtless', 'woman', 'mother', 'man', 'men', 'man', 'people', 'player', 'woman', 'football', 'teenagers', 'man', 'dog', 'lady', 'African', 'light', 'man', 'of', 'young', 'in', 'women', 'people', 'men', 'man', 'man', 'people', 'baseball', 'large', 'Asian', 'woman', 'children', '<unk>', 'guy', 'professionally', 'boy', 'men', 'man', 'rhythmic', 'a', 'lady', '<unk>']\n",
      "['A', 'Five', 'A', 'A', 'The', 'A', 'A', 'A', 'Two', 'Two', 'People', 'Two', 'Three', 'Children', 'Runners', 'Two', 'The', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'People', 'Men', 'A', 'A', 'A', 'A', 'A', 'Two', 'A', 'A', 'Two', 'A', 'Two', 'A', 'An', 'A', 'A', 'Two', 'A', 'A', 'A', 'An', 'A', 'A', 'A', 'A', 'A', 'Two', 'Kids', 'A', 'A', 'An', 'Two', 'A', 'A', 'Two', 'A', 'A', 'A', 'Several', 'People', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'The', 'The', 'A', 'A', 'A', 'Two', 'The', 'People', 'Men', 'Two', 'A', 'A', 'A', 'A', 'Three', 'A', 'Two', 'This', 'A', 'A', 'Three', 'A', 'A', 'A', 'An', 'A', 'A', 'A', 'A', 'A', 'Two', 'Two', 'Two', 'A', 'A', 'Many', 'Baseball', 'A', 'An', 'A', 'Two', 'A', 'A', 'A', 'A', 'Two', 'A', 'A', 'Two', 'The', 'Two', 'man', 'people', 'cowboy', 'middle', 'girl', 'woman', 'man', 'girl', 'German', 'boys', 'walk', 'men', 'boys', 'are', 'are', 'men', '#', 'black', 'person', 'man', 'man', 'person', 'little', 'person', 'man', 'sit', 'playing', 'heron', 'woman', 'middle', 'man', 'man', 'workers', 'little', 'group', 'people', 'street', 'male', 'man', 'Asian', 'boy', 'little', 'children', 'man', 'red', 'man', 'elderly', 'boy', 'woman', 'male', 'baby', 'man', 'women', 'jump', 'woman', 'bull', 'Asian', 'men', 'man', 'woman', 'men', 'woman', 'brown', 'mountain', 'construction', 'walk', 'man', 'young', 'man', 'man', 'small', 'little', 'young', 'boy', 'cat', 'man', 'well', 'two', 'group', 'woman', 'boy', 'small', 'boys', 'gentleman', 'sitting', ',', 'men', 'man', 'woman', 'mother', 'man', 'men', 'man', 'people', 'male', 'woman', 'football', 'teenagers', 'man', 'dog', 'woman', 'African', 'worker', 'man', 'group', 'small', 'man', 'women', 'people', 'men', 'man', 'man', 'people', 'players', 'large', 'Asian', 'woman', 'children', 'climber', 'guy', 'woman', 'boy', 'men', 'man', 'female', 'men', 'lady', 'college', 'in', 'are', 'is', '-', 'is', 'is', 'holding', 'in', 'Shepherd', 'are', 'down', 'attempt', 'are', 'fighting', 'in', 'are', 'eight', 'and', 'in', 'in', 'is', 'with', 'child', 'is', 'wearing', 'in', 'playing', 'jumps', 'is', '-', 'is', 'riding', 'are', 'girl', 'of', 'are', 'is', 'hikers', 'is', 'man', 'plays', 'boy', 'are', 'with', 'in', 'in', 'man', 'jumps', 'is', 'is', 'in', 'in', 'and', 'in', 'is', 'is', 'man', 'in', 'in', 'in', 'are', 'woman', 'dog', 'biker', 'workers', 'down', 'sitting', 'girl', 'is', 'with', 'child', 'girl', 'girl', 'in', 'is', 'in', '-', 'dogs', 'of', 'in', 'poses', 'black', 'play', 'is', 'on', 'in', 'with', 'in', 'sits', 'and', 'in', 'in', 'is', 'sitting', 'wearing', 'in', 'player', 'are', 'in', 'jumps', 'with', 'American', 'with', 'in', 'of', 'child', 'in', 'in', 'look', 'working', 'is', 'lays', 'are', 'baseball', 'group', 'man', 'with', 'are', 'with', 'is', 'in', 'in', ',', 'drinking', 'dancer', 'are', 'with', 'pose', 'a', 'gathered', 'trying', 'aged', 'drinking', 'a', 'a', 'white', 'are', 'a', 'a', 'to', 'in', 'to', 'a', 'their', 'is', 'white', 'a', 'jeans', 'a', 'a', 'in', 'his', 'a', 'front', 'a', 'over', 'a', 'aged', 'pouring', 'his', 'on', 'holds', 'children', 'in', 'beside', 'are', 'a', 'in', 'playing', 'is', 'a', 'a', 'red', 'a', 'in', 'jumps', 'a', 'his', 'the', 'a', 'a', 'a', 'in', 'jumping', 'sits', 'camouflage', 'a', 'a', 'playing', 'uses', 'is', 'is', 'in', 'a', 'on', 'is', 'doing', 'clothes', 'is', 'is', 'is', 'a', 'on', 'a', 'aged', ',', 'people', 'a', 'posing', 'dog', 'soccer', 'the', 'a', 'playing', 'hats', 'a', 'in', 'her', 'a', 'traditional', 'riding', 'on', 'a', 'a', 'in', 'on', 'a', 'over', 'a', 'man', 'a', 'a', 'young', 'in', 'a', 'dresses', 'at', 'working', 'fixing', 'on', 'on', 'player', 'of', 'is', 'her', 'through', 'a', 'shaking', 'a', 'a', 'one', 'a', 'in', 'on', 'a', 'for', 'red', 'around', 'to', 'man', 'a', 'a', 'another', 'and', 'each', 'street', 'street', 'catch', 'their', 'the', 'a', 'their', '\"', 'dog', 'red', 'is', 'a', 'costume', 'blue', 'his', 'gray', 'of', 'a', 'a', 'group', 'man', 'a', 'bike', 'a', 'a', 'play', 'the', 'a', 'harnessed', 'a', 'a', 'a', 'on', 'a', 'a', 'and', 'black', 'a', 'to', 'picture', 'his', 'camera', 'white', 'man', 'pool', 'a', 'to', 'in', 'and', 'hat', 'blue', 'in', 'a', 'a', 'a', 'orange', 'city', 'a', 'her', 'a', 'is', 'a', 'a', 'while', 'red', 'a', 'gray', 'man', 'one', 'people', 'purple', 'with', 'jumping', 'ball', 'picture', 'bench', 'in', 'and', 'black', 'a', 'young', 'black', 'clothing', 'a', 'a', 'blue', 'white', 'a', 'a', 'white', 'a', 'is', 'stands', 'is', 'red', 'boys', 'green', 'red', 'are', 'a', 'in', 'the', 'a', 'a', 'getting', 'people', 'outside', 'cellphone', 'through', 'blue', 'another', 'woman', 'a', 'in', 'a', 'a', 'a', 'black', 'a', 'shirt', 'a', 'climb', 'sits', 'fountain', 'on', 'with', 'a', 'other', 'street', 'street', 'a', 'red', 'ball', 'day', 'while', 'pleasure', 'playing', 'jacket', 'playing', 'a', 'stands', 'looks', 'arms', 'shirt', 'a', 'field', 'fence', 'of', 'with', 'a', 'bike', 'brick', 'little', 'in', ',', 'a', 'to', 'bridge', 'picture', 'toy', 'a', 'small', 'group', 'white', 'shirt', 'blue', 'hit', 'of', 'family', 'looking', 'shirt', 'are', 'in', 'a', 'a', 'the', 'and', 'is', 'shirt', 'front', 'large', 'a', 'a', 'vests', 'street', 'table', 'her', 'trick', 'welding', 'with', 'a', 'a', 'shirt', 'train', 'shirt', 'in', 'one', 'people', 'sweater', 'a', 'over', '.', 'of', 'bench', 'a', 'walking', 'black', 'chair', 'son', 'jacket', ',', 'bicycle', 'grass', 'helmet', 'sweater', 'orange', 'train', 'shirt', 'pond', 'with', 'in', 'a', 'suit', 'wait', 'and', 'shirt', 'standing', 'city', 'a', 'mountain', 'couch', 'street', 'ready', 'are', 'at', 'as', 'a', 'helmet', 'lady', 'standing', 'shirt', 'black', 'while', 'blue', 'a', ',', 'picture', 'is', 'in', 'the', 'sitting', 'fountain', 'a', 'his', 'a', '.', 'street', 'with', 'water', 'on', '.', 'in', 'while', 'to', 'playing', 'and', 'a', 'a', 'in', 'looking', '.', 'and', 'building', '.', '.', 'a', 'a', 'a', '.', 'building', 'boy', 'a', 'one', 'with', 'a', 'over', 'of', 'fountain', 'a', 'small', 'of', 'pants', 'is', 'coat', 'a', 'a', 'behind', 'at', 'and', 'looking', 'a', 'in', 'ball', 'street', 'standing', 'playing', 'and', 'of', 'piece', 'toy', 'a', 'vests', 'in', 'table', 'out', '.', 'a', 'another', 'a', 'a', 'and', '.', 'jumps', 'a', 'with', 'on', 'and', 'large', 'the', '\\n', 'a', 'in', 'man', 'by', 'shorts', 'in', 'enjoying', 'plays', 'and', 'bike', 'and', ',', 'is', 'jersey', 'to', 'looking', '.', 'her', 'the', 'a', 'suit', 'for', 'playing', 'is', 'with', 'city', 'white', '.', 'with', 'street', 'to', 'gathered', 'night', 'she', 'small', 'is', '.', 'on', 'and', 'and', 'while', 'and', ',', 'glasses', '.', 'walking', 'a', 'the', 'and', '.', 'field', 'arm', 'green', '\\n', 'a', 'a', 'from', 'a', '\\n', 'the', 'while', 'the', 'a', 'black', 'beach', 'a', 'front', 'looking', '\\n', 'jeans', 'in', '\\n', '\\n', 'group', 'hair', 'a', '\\n', '.', 'boy', 'circle', 'in', 'many', 'fence', 'the', 'a', 'with', '.', 'across', 'people', 'is', 'sitting', 'is', 'tennis', 'baby', 'his', 'the', 'a', 'at', 'pool', 'a', '.', 'with', 'next', 'the', 'white', 'a', 'of', 'in', '.', 'are', 'a', 'with', 'of', '\\n', 'a', 'person', '.', 'a', 'black', '\\n', 'over', 'man', 'a', 'a', 'a', 'large', '.', '<eos>', 'woman', 'a', ',', 'a', 'stands', 'a', 'a', 'a', 'walking', 'a', 'water', 'is', 'putting', 'and', 'be', 'at', '\\n', 'face', 'middle', 'a', 'is', 'a', 'in', 'a', 'other', 'city', 'white', '\\n', 'a', '.', 'a', 'and', '.', 'walks', 'fence', 'being', '\\n', 'a', 'a', 'and', 'a', 'pink', 'some', 'and', '\\n', 'a', 'park', '.', 'playing', '\\n', '.', '.', 'green', '<eos>', 'a', 'street', 'a', 'a', '<eos>', 'street', 'while', 'the', 'white', 'pants', 'with', 'a', 'of', 'in', '<eos>', 'and', 'the', '<eos>', '<eos>', 'of', 'and', 'kitchen', '<eos>', '\\n', 'on', 'circle', 'a', 'flags', 'in', 'ocean', 'group', 'a', '.', 'a', '.', 'doing', 'at', 'on', 'ball', 'in', '.', 'camera', 'blue', 'a', '.', 'water', '\\n', 'the', 'to', 'street', 'shorts', 'large', 'a', 'a', '\\n', 'on', 'sunny', 'a', 'a', '<eos>', '.', '.', '\\n', '.', 'shorts', '<eos>', 'the', 'is', 'a', 'a', 'apron', 'green', '.', '<eos>', 'in', 'city', 'one', 'body', 'on', 'a', 'beautiful', 'guitar', 'down', 'a', 'and', 'a', 'on', 'holding', '.', 'a', '<eos>', 'with', 'of', 'a', 'dancing', 'to', 'a', 'a', 'in', 'park', 'white', '<eos>', 'hospital', '\\n', 'a', 'and', '\\n', 'by', '.', 'up', '<eos>', 'and', 'shirt', 'and', 'a', 'outfit', 'kind', 'and', '<eos>', 'a', 'with', '.', 'the', '<eos>', '\\n', '\\n', 'walks', '<eos>', 'while', '.', 'water', '.', '<eos>', '.', 'women', 'hand', 'ball', 'is', 'a', 'a', 'a', 'the', '<eos>', 'a', 'grass', '<eos>', '<eos>', 'a', 'glasses', '.', '<eos>', '<eos>', 'his', 'while', 'a', '.', 'the', '.', 'of', 'girl', '\\n', 'a', '\\n', 'a', 'a', 'the', 'with', 'a', '.', '.', 'shirt', 'book', '\\n', 'water', '<eos>', 'street', 'a', '.', 'is', 'crowd', 'a', 'tree', '<eos>', 'the', 'day', 'with', 'a', '<eos>', '\\n', '\\n', '<eos>', '.', 'shorts', '<eos>', 'sand', 'sitting', 'in', 'a', 'is', 'with', '\\n', '<eos>', 'a', ',', 'man', 'of', 'a', 'with', 'day', '.', 'the', '.', 'a', ',', 'a', 'a', '.', 'window', '<eos>', 'a', 'the', '.', 'with', 'something', 'puddle', 'dog', 'in', '.', '.', '<eos>', '.', '<eos>', 'football', 'sitting', '<eos>', '.', '\\n', 'on', '<eos>', 'and', 'and', 'one', 'book', 'is', 'of', 'yellow', '<eos>', 'white', 'a', '\\n', 'accordion', '<eos>', '<eos>', '<eos>', 'by', '<eos>', 'a', '\\n', '.', '\\n', '<eos>', '\\n', 'watch', 'to', '.', 'holding', 'tennis', '.', 'building', 'water', '<eos>', 'green', '.', '<eos>', '<eos>', '.', 'holding', '\\n', '<eos>', 'building', 'lap', 'a', 'and', '\\n', 'edge', '\\n', 'a', 'girl', '<eos>', '.', '<eos>', 'on', 'desk', 'side', 'his', 'pink', '\\n', '\\n', 'is', '.', '<eos>', 'with', '<eos>', '.', 'large', '\\n', 'playing', '.', 'and', '.', '<eos>', 'floor', '.', 'a', '.', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', 'is', '<eos>', 'on', 'on', 'a', '.', 'eating', 'with', '<eos>', '<eos>', 'blue', ',', ',', 'water', 'grassy', 'a', '.', '\\n', 'street', '\\n', 'body', ',', '.', 'football', '\\n', 'of', '<eos>', 'picture', 'background', '.', 'a', 'to', '.', '.', 'shallow', '\\n', '.', '<eos>', '.', '<eos>', '.', 'outside', '<eos>', '\\n', '<eos>', 'the', '<eos>', 'and', 'holding', 'in', '.', 'performing', 'horses', 'and', '<eos>', 'and', '.', '<eos>', '.', '<eos>', '<eos>', '<eos>', 'a', '<eos>', 'a', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '.', 'the', '\\n', 'her', 'ball', '\\n', '.', 'in', '<eos>', 'and', '\\n', '<eos>', '<eos>', '.', 'a', '<eos>', '<eos>', '.', '.', 'adult', 'standing', '<eos>', 'of', '<eos>', 'in', 'watches', '<eos>', '\\n', '<eos>', 'a', 'with', 'of', 'racket', 'pink', '<eos>', '<eos>', 'a', '\\n', '<eos>', 'a', '<eos>', '\\n', 'large', '<eos>', 'tennis', '\\n', 'a', '\\n', '<eos>', '.', '\\n', 'electronic', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'standing', '<eos>', 'the', 'a', 'ball', '\\n', 'a', 'her', '<eos>', '<eos>', 'shirt', ',', 'one', '.', 'shore', 'arms', '\\n', '<eos>', '.', '<eos>', 'of', ',', '.', '.', '<eos>', 'a', '<eos>', 'of', 'in', '\\n', 'microphone', 'something', '\\n', '\\n', 'water', '<eos>', '\\n', '<eos>', '\\n', '<eos>', '\\n', '.', '<eos>', '<eos>', '<eos>', '.', '<eos>', 'food', 'a', 'red', '\\n', 'a', 'and', 'looking', '<eos>', 'white', '.', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', 'blue', '<eos>', 'soccer', '\\n', 'hole', '<eos>', '<eos>', '<eos>', '\\n', 'the', '<eos>', '.', '.', '<eos>', '\\n', 'the', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '\\n', 'a', '<eos>', '<eos>', '\\n', '\\n', 'watches', 'on', '<eos>', 'the', '<eos>', 'a', 'girl', '<eos>', '<eos>', '<eos>', 'a', 'a', 'the', '.', 'hat', '<eos>', '<eos>', 'a', '<eos>', '<eos>', 'water', '<eos>', '<eos>', 'large', '<eos>', '.', '<eos>', ',', '<eos>', '<eos>', '\\n', '<eos>', 'device', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'in', '<eos>', 'sand', 'cigarette', 'in', '<eos>', 'table', '.', '<eos>', '<eos>', '.', 'and', 'man', '\\n', 'next', '.', '<eos>', '<eos>', '\\n', '<eos>', 'water', ',', '\\n', '\\n', '<eos>', 'window', '<eos>', 'a', 'the', '<eos>', '.', '.', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '.', '<eos>', 'or', 'book', 'and', '<eos>', 'hula', 'and', 'at', '<eos>', ',', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'blue', '<eos>', 'ball', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', 'person', '<eos>', '.', '\\n', '<eos>', '<eos>', 'water', '<eos>', 'white', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '.', 'a', '<eos>', 'ground', '<eos>', 'blue', 'watches', '<eos>', '<eos>', '<eos>', '.', 'desk', 'side', '\\n', ',', '<eos>', '<eos>', '.', '<eos>', '<eos>', '.', '<eos>', '<eos>', 'large', '<eos>', '\\n', '<eos>', 'while', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '.', '.', 'the', '<eos>', 'with', '\\n', '<eos>', '<eos>', '\\n', 'a', 'is', '<eos>', 'to', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '.', ',', '<eos>', '<eos>', '<eos>', '.', '<eos>', 'desk', 'background', '<eos>', '\\n', '\\n', '<eos>', '.', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', 'something', '.', 'playing', '<eos>', 'hoops', 'horses', 'the', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '.', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '\\n', 'in', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '.', '<eos>', 'hat', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', 'street', '<eos>', '.', '<eos>', ',', '.', '<eos>', '<eos>', '<eos>', '.', 'in', 'of', '<eos>', 'a', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '\\n', '\\n', 'grass', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', 'and', 'her', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '\\n', 'and', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '.', '.', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '\\n', 'the', '<eos>', '.', '.', 'a', '<eos>', 'red', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', ',', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '\\n', '<eos>', 'and', '\\n', '<eos>', '<eos>', '<eos>', '\\n', 'a', 'a', '<eos>', 'man', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', 'on', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', ',', '<eos>', 'table', '<eos>', '<eos>', '<eos>', '.', 'and', 'feet', '<eos>', 'large', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '\\n', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', 'sand', '<eos>', '\\n', '\\n', 'while', '<eos>', ',', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'white', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'table', 'car', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', 'running', '<eos>', '.', '<eos>', '<eos>', '<eos>', '\\n', 'a', 'in', '<eos>', 'body', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', 'sitting', '<eos>', 'is', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', 'in', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'guitar', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'woman', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '.', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'through', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', 'and', 'the', '<eos>', 'of', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'and', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', 'on', '<eos>', 'looking', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'and', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'in', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', 'air', '<eos>', 'water', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', 'at', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'white', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'grass', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', 'in', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'bench', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'guitar', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'woman', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'in', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'in', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', 'air', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'background', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'in', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'in', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'the', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'background', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'woman', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'in', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'a', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'woman', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '.', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '\\n', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def predict(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            test_idx = 2\n",
    "            print([de_vocab.itos[i] for i in src[test_idx]])\n",
    "            print([en_vocab.itos[i] for i in trg[test_idx]])\n",
    "            \n",
    "            \n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            print([en_vocab.itos[i] for i in output.argmax(1)])\n",
    "            trg = trg[1:].view(-1)\n",
    "            break\n",
    "\n",
    "predict(model, test_iter, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
