{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a765d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d358b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return(log_probs)\n",
    "    \n",
    "#\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_word_vec(word,word_to_ix,device=\"cpu\"):\n",
    "    context_ids = make_context_vector(word, word_to_ix).to(device)\n",
    "    assert context_ids.is_cuda is True # returns a boolean\n",
    "    return model.embeddings(context_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ed1383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: {'computers.', 'computer', 'inhabit', 'As', 'evolution', 'program.', 'computational', 'direct', 'effect,', 'Computational', 'rules', 'process', 'things', 'processes.', 'study', 'processes', 'to', 'called', 'our', 'pattern', 'of', 'that', 'spirits', 'is', 'process.', 'a', 'about', 'The', 'We', 'data.', 'we', 'abstract', 'beings', 'by', 'People', 'conjure', 'spells.', 'they', 'evolve,', 'are', 'with', 'manipulate', 'programs', 'idea', 'other', 'In', 'directed', 'the', 'create'}\n",
      "data:[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n",
      "make_context_vector: tensor([28, 39, 16, 14])\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"vocab: {vocab}\")\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(f\"data:{data[:5]}\")\n",
    "\n",
    "#\n",
    "\n",
    "print(f\"make_context_vector: {make_context_vector(data[0][0], word_to_ix)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2c457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "device = torch.device('cuda:0')\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(len(vocab), embedding_dim=10, context_size=CONTEXT_SIZE*2)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17803a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9782,  1.1687, -1.5671, -1.0935,  0.0887,  0.7096, -1.6064, -0.1839,\n",
       "          0.8062, -0.3231]], device='cuda:0', grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_vec([data[0][0][0]],word_to_ix,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32bef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([246.4905]), tensor([134.8672]), tensor([35.3014]), tensor([3.1480]), tensor([1.5589]), tensor([1.1555]), tensor([0.9227]), tensor([0.7691]), tensor([0.6592]), tensor([0.5765])]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in data:\n",
    "        context_ids = make_context_vector(context, word_to_ix)\n",
    "        context_ids = context_ids.to(device)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_ids)\n",
    "        label = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "        label = label.to(device)\n",
    "        loss = loss_function(log_probs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14d24328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0039,  1.1754, -1.5717, -1.1152,  0.1006,  0.7099, -1.6132, -0.1806,\n",
       "          0.8196, -0.3096]], device='cuda:0', grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_vec([data[0][0][0]],word_to_ix,device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb8ec5",
   "metadata": {},
   "source": [
    "## 中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1e3a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: {'透', '志', '持', '任', '地', '心', '示', '管', '外', '問', '揮', '但', '前', '院', '診', '浩', '署', '到', '指', '諾', '福', '關', '共', '、', '飯', '時', '記', '者', '接', '觸', '4', '用', '潔', '疫', '於', '大', '感', '商', '館', '媒', '今', '餐', '能', '；', '中', '修', '「', '露', '續', '，', '作', '陳', '清', '華', '表', '航', '不', '否', '情', '明', '釐', '攜', '何', '有', '方', '會', '台', '可', '參', '查', '。', '0', '曾', '6', '去', '或', '追', '午', '和', '長', '增', '公', '手', '層', '店', '受', '周', '訪', '央', '3', '抗', '的', '沒', '性', '？', '染', '富', '首', '水', '天', '部', '還', '客', '雙', 'B', '1', '案', '員', '種', '過', '除', '回', '一', '新', '衛', '是', '此', '昨', '飲', '疾', '各', '與', '需', '被', '包', '聚', '源', '應', '因', '工', '施', '確', '）', '由', '5', '2', '都', '整', '（', '」', '排', '同', '上', '分', '體', '特', '佈', '例', '加', '流', '行', '人', '旅', '要', '醫', '在', '樓', '群', '相', '電'}\n",
      "data:[(['華', '航', '富', '特'], '諾'), (['航', '諾', '特', '飯'], '富'), (['諾', '富', '飯', '店'], '特'), (['富', '特', '店', '群'], '飯'), (['特', '飯', '群', '聚'], '店')]\n",
      "make_context_vector: tensor([ 53,  55,  96, 145])\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = [t for t in \"華航諾富特飯店群聚案新增首例外包商水電工案1145確診，中央流行疫情指揮中心持續追查水電工感染源，衛福部長陳時中與疾管署長周志浩今天上午一同到台大醫院參加「清潔雙手，攜手抗疫」記者會，會前陳時中受訪透露，案1145可能曾到過諾富特飯店一館B1用餐，不排除任何感染可能。指揮中心昨天公佈的案1145，是在諾富特飯店的一館3、5、6樓層（整修樓層）工作，由於和飯店人員、旅客都沒有接觸，因此感染源不明；不過陳時中今天表示，在施工的地方大部分都是工作人員，但是可能有在B1的地方共餐。媒體追問，是否表示案1145有去過B1，是不是可能被案1120或是餐飲部的員工感染？陳時中回應，各種可能性都有，但還需要相關的釐清。\"]\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"vocab: {vocab}\")\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(f\"data:{data[:5]}\")\n",
    "\n",
    "#\n",
    "\n",
    "print(f\"make_context_vector: {make_context_vector(data[0][0], word_to_ix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20b3a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "device = torch.device('cuda:0')\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(len(vocab), embedding_dim=10, context_size=CONTEXT_SIZE*2)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "268ba0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1458.3297]), tensor([890.8279]), tensor([376.6428]), tensor([125.4319]), tensor([141.4768]), tensor([135.7821]), tensor([59.3746]), tensor([89.7570]), tensor([68.6293]), tensor([14.3338])]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in data:\n",
    "        context_ids = make_context_vector(context, word_to_ix)\n",
    "        context_ids = context_ids.to(device)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_ids)\n",
    "        label = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "        label = label.to(device)\n",
    "        loss = loss_function(log_probs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19945f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:['華'], vec: tensor([[-0.6937, -0.4363,  0.2154, -2.3111, -1.4362,  0.1475,  0.5455,  1.2378,\n",
      "         -1.5731,  0.3879]], device='cuda:0', grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"text:{[data[0][0][0]]}, vec: {get_word_vec([data[0][0][0]],word_to_ix,device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e090ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
